{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "class SWDataset(Dataset):\n",
        "    \"\"\"A custom dataset to read the shallow water (SW) simulation data.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        file_path,\n",
        "        order=1,\n",
        "        numtime=1200,\n",
        "        mode=\"train\",\n",
        "        train_frac=0.5,\n",
        "        valid_frac=0.2,\n",
        "        normalize=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Parameters:\n",
        "            - file_path (str): Path to the data file.\n",
        "            - order (int): Autoregressive model order.\n",
        "            - numtime (int): Total number of time steps in the data.\n",
        "            - mode (str): Mode of operation (\"train\", \"valid\", \"test\").\n",
        "            - train_frac (float): Fraction of data to use for training.\n",
        "            - valid_frac (float): Fraction of data to use for validation.\n",
        "        \"\"\"\n",
        "        assert mode in [\n",
        "            \"train\",\n",
        "            \"valid\",\n",
        "            \"test\",\n",
        "        ], \"Mode should be either 'train', 'valid', or 'test'\"\n",
        "\n",
        "        super(SWDataset, self).__init__()\n",
        "\n",
        "        self.file_path = file_path\n",
        "        self.order = order\n",
        "        self.numtime = numtime\n",
        "        self.mode = mode\n",
        "\n",
        "        # Determine split indices based on dataset size and provided fractions\n",
        "        total_samples = self.numtime - 1 - self.order\n",
        "        self.train_end = int(train_frac * total_samples)\n",
        "        self.valid_end = self.train_end + int(valid_frac * total_samples)\n",
        "\n",
        "        # Lists to store elevation and velocity data for normalization\n",
        "        zetas = []\n",
        "        velocities = []\n",
        "\n",
        "        # Read data from HDF5 file\n",
        "        with h5py.File(self.file_path, \"r\") as hdf_file:\n",
        "            self.init_zeta = torch.tensor(hdf_file[f\"timestep_0\"]['elevation'][:])\n",
        "            self.init_vel = torch.tensor(hdf_file[f\"timestep_0\"]['velocity'][:])\n",
        "            for index in range(self.numtime):\n",
        "                zetas.append(hdf_file[f\"timestep_{index}\"][\"elevation\"][:])\n",
        "                velocities.append(hdf_file[f\"timestep_{index}\"][\"velocity\"][:])\n",
        "\n",
        "        # Convert list to numpy array for easier operations\n",
        "        zetas = np.array(zetas)\n",
        "        velocities = np.array(velocities)\n",
        "\n",
        "        # Calculate statistics (mean, std, min, max) for normalization\n",
        "        self.zeta_min, self.velocity_min = 0.0, 0.0\n",
        "        self.zeta_max, self.velocity_max = 1.0, 1.0\n",
        "        if normalize:\n",
        "          self.zeta_min = zetas.min()\n",
        "          self.zeta_max = zetas.max()\n",
        "          self.velocity_min = velocities.min()\n",
        "          self.velocity_max = velocities.max()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the length of the dataset based on mode.\"\"\"\n",
        "        if self.mode == \"train\":\n",
        "            return self.train_end\n",
        "        elif self.mode == \"valid\":\n",
        "            return self.valid_end - self.train_end\n",
        "        else:  # mode is 'test'\n",
        "            return self.numtime - 1 - self.order - self.valid_end\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Return the data at the given index.\"\"\"\n",
        "        # Adjust the index based on mode (train/valid/test)\n",
        "        if self.mode == \"valid\":\n",
        "            index += self.train_end\n",
        "        elif self.mode == \"test\":\n",
        "            index += self.valid_end\n",
        "\n",
        "        zetas, velocities = [], []\n",
        "\n",
        "        # Read data for the given index from the HDF5 file\n",
        "        with h5py.File(self.file_path, \"r\") as hdf_file:\n",
        "            for i in range(self.order):\n",
        "                # Normalize Elevation and velocity using min-max scaling\n",
        "                zeta = (\n",
        "                    hdf_file[f\"timestep_{index + i}\"][\"elevation\"][:]\n",
        "                    - self.zeta_min\n",
        "                ) / (self.zeta_max - self.zeta_min)\n",
        "                vel = (\n",
        "                    2\n",
        "                    * (\n",
        "                        (\n",
        "                            hdf_file[f\"timestep_{index + i}\"][\"velocity\"][:]\n",
        "                            - self.velocity_min\n",
        "                        )\n",
        "                        / (self.velocity_max - self.velocity_min)\n",
        "                    )\n",
        "                    - 1\n",
        "                )\n",
        "\n",
        "                # Add an extra dimension to match the expected input shape\n",
        "                zetas.append(zeta[None, :])\n",
        "                velocities.append(vel[None, :])\n",
        "\n",
        "            # Extract and normalize target data for the given index\n",
        "            target_zeta = (\n",
        "                hdf_file[f\"timestep_{index + self.order}\"][\"elevation\"][:]\n",
        "                - self.zeta_min\n",
        "            ) / (self.zeta_max - self.zeta_min)\n",
        "            target_vel = (\n",
        "                2\n",
        "                * (\n",
        "                    (\n",
        "                        hdf_file[f\"timestep_{index + self.order}\"][\"velocity\"][:]\n",
        "                        - self.velocity_min\n",
        "                    )\n",
        "                    / (self.velocity_max - self.velocity_min)\n",
        "                )\n",
        "                - 1\n",
        "            )\n",
        "\n",
        "        # Concatenate the input data\n",
        "        input_zeta = np.concatenate(zetas, axis=0)\n",
        "        input_vel = np.concatenate(velocities, axis=0)\n",
        "\n",
        "        # Convert input and target data to PyTorch tensors\n",
        "        return (\n",
        "            torch.tensor(input_zeta, dtype=torch.float32),\n",
        "            torch.tensor(input_vel, dtype=torch.float32),\n",
        "        ), (\n",
        "            torch.tensor(\n",
        "                target_zeta[None, :], dtype=torch.float32\n",
        "            ),  # Add an extra dimension\n",
        "            torch.tensor(\n",
        "                target_vel[None, :], dtype=torch.float32\n",
        "            ),  # Add an extra dimension\n",
        "        )\n",
        "\n",
        "    def get_initial_conditions(self):\n",
        "        return self.init_zeta.unsqueeze(0).unsqueeze(0), self.init_vel.unsqueeze(0).unsqueeze(0)"
      ],
      "metadata": {
        "id": "HgK1i4mjjoG0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}