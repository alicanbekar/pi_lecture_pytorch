{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Explanation\n",
    "\n",
    "1. **__init__ Method**: This method initializes the dataset with necessary parameters. It also reads data from the HDF5 file and calculates the normalization statistics.\n",
    "2. **__len__ Method**: Returns the length of the dataset, which varies based on whether it's in training, validation, or testing mode.\n",
    "3. **__getitem__ Method**: Provides a way to access individual data items. This method reads data for a given index, normalizes it, and returns it in the required format.\n",
    "4. **get_initial_conditions Method**: Returns the initial conditions of the simulation, useful for usupervised learning tasks.\n",
    "\n",
    "With this custom dataset, we've built the foundation for reading and preprocessing the SW simulation data. In the next sections, we'll build our model, loss function, and the training loop to complete our PyTorch project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(datafile,\n",
    "                dtype=torch.float32,\n",
    "                train_frac=0.5,\n",
    "                valid_frac=0.2,\n",
    "                normalize=True,\n",
    "                max_sims=None):\n",
    "    with np.load(datafile) as fid:\n",
    "        zeta, v = fid['zeta'], fid['v']\n",
    "    \n",
    "    if max_sims is not None:\n",
    "        zeta, v = zeta[:max_sims], v[:max_sims]\n",
    "        \n",
    "    # divide experiments (not time points) randomly into train/test/val\n",
    "    n_exp = zeta.shape[0]\n",
    "    exp_order = np.random.permutation(n_exp)\n",
    "       \n",
    "    n_train = int(n_exp * train_frac)\n",
    "    n_val = int(n_exp * valid_frac)\n",
    " \n",
    "    stats = dict(mz=zeta[exp_order[:n_train]].mean(), \n",
    "                 sdz=zeta[exp_order[:n_train]].std(),\n",
    "                 mv=v[exp_order[:n_train]].mean(),\n",
    "                 sdv=v[exp_order[:n_train]].std())\n",
    "    if normalize:\n",
    "        zeta, v = (zeta - stats['mz']) / stats['sdz'], (v - stats['mv']) / stats['sdv']\n",
    "\n",
    "    train_data = make_dataset(zeta[exp_order[:n_train]], v[exp_order[:n_train]], dtype=dtype)\n",
    "\n",
    "    val_data = make_dataset(zeta[exp_order[n_train:n_train+n_val]], v[exp_order[n_train:n_train+n_val]], dtype=dtype)\n",
    "    test_data = make_dataset(zeta[exp_order[n_train+n_val:]], v[exp_order[n_train+n_val:]], dtype=dtype)      \n",
    "    \n",
    "    return train_data, val_data, test_data, stats\n",
    "\n",
    "def make_dataset(zeta, v, dtype=torch.float32):        \n",
    "    zeta_in = zeta[:, :-1, :].reshape(-1, zeta.shape[-1])\n",
    "    zeta_out = zeta[:, 1:, :].reshape(-1, zeta.shape[-1])\n",
    "    \n",
    "    v_in = v[:, :-1, :].reshape(-1, v.shape[-1])\n",
    "    v_out = v[:, 1:, :].reshape(-1, v.shape[-1])\n",
    "    \n",
    "    tensors = [torch.tensor(x, dtype=dtype).unsqueeze(1) for x in [zeta_in, zeta_out, v_in, v_out]]    \n",
    "    return TensorDataset(*tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = 'data.npz'\n",
    "train_data, val_data, test_data, stats = import_data(datafile)\n",
    "with np.load(datafile) as fid:\n",
    "    nsteps = fid['zeta'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SWDataset(Dataset):\n",
    "    \"\"\"A custom dataset to read the shallow water (SW) simulation data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path,\n",
    "        order=1,\n",
    "        numtime=1200,\n",
    "        mode=\"train\",\n",
    "        train_frac=0.5,\n",
    "        valid_frac=0.2,\n",
    "        normalize=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        Parameters:\n",
    "            - file_path (str): Path to the data file.\n",
    "            - order (int): Autoregressive model order.\n",
    "            - numtime (int): Total number of time steps in the data.\n",
    "            - mode (str): Mode of operation (\"train\", \"valid\", \"test\").\n",
    "            - train_frac (float): Fraction of data to use for training.\n",
    "            - valid_frac (float): Fraction of data to use for validation.\n",
    "        \"\"\"\n",
    "        assert mode in [\n",
    "            \"train\",\n",
    "            \"valid\",\n",
    "            \"test\",\n",
    "        ], \"Mode should be either 'train', 'valid', or 'test'\"\n",
    "\n",
    "        super(SWDataset, self).__init__()\n",
    "\n",
    "        self.file_path = file_path\n",
    "        self.order = order\n",
    "        self.numtime = numtime\n",
    "        self.mode = mode\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # Determine split indices based on dataset size and provided fractions\n",
    "        total_samples = self.numtime - 1 - self.order\n",
    "        self.train_end = int(train_frac * total_samples)\n",
    "        self.valid_end = self.train_end + int(valid_frac * total_samples)\n",
    "\n",
    "        # Lists to store elevation and velocity data for normalization\n",
    "        zetas = []\n",
    "        velocities = []\n",
    "\n",
    "        # Read data from HDF5 file\n",
    "        with h5py.File(self.file_path, \"r\") as hdf_file:\n",
    "            self.init_zeta = torch.tensor(hdf_file[f\"timestep_0\"]['elevation'][:])\n",
    "            self.init_vel = torch.tensor(hdf_file[f\"timestep_0\"]['velocity'][:])\n",
    "            for index in range(self.numtime):\n",
    "                zetas.append(hdf_file[f\"timestep_{index}\"][\"elevation\"][:])\n",
    "                velocities.append(hdf_file[f\"timestep_{index}\"][\"velocity\"][:])\n",
    "\n",
    "        # Convert list to numpy array for easier operations\n",
    "        zetas = np.array(zetas)\n",
    "        velocities = np.array(velocities)\n",
    "\n",
    "        # Calculate statistics (mean, std, min, max) for (optional) normalization\n",
    "        self.zeta_min = zetas.min()\n",
    "        self.zeta_max = zetas.max()\n",
    "        self.velocity_min = velocities.min()\n",
    "        self.velocity_max = velocities.max()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the dataset based on mode.\"\"\"\n",
    "        if self.mode == \"train\":\n",
    "            return self.train_end\n",
    "        elif self.mode == \"valid\":\n",
    "            return self.valid_end - self.train_end\n",
    "        else:  # mode is 'test'\n",
    "            return self.numtime - 1 - self.order - self.valid_end\n",
    "        \n",
    "    def normalize_vals(self, z, v):\n",
    "        if self.normalize:\n",
    "            z = (z - self.zeta_min) / (self.zeta_max - self.zeta_min)\n",
    "            v = 2.0 * (v - self.velocity_min) / (self.velocity_max - self.velocity_min) - 1.0\n",
    "        return z, v\n",
    "        \n",
    "    def denormalize_vals(self, zeta, velocity):\n",
    "        if self.normalize:\n",
    "            z = z * (zelf.zeta_max - self.zeta_min) + self.zeta_min \n",
    "            v = (v + 1.0) * 0.5 * (self.velocity_max - self.velocity_min) + self.velocity_min\n",
    "        return z, v\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return the data at the given index.\"\"\"\n",
    "        # Adjust the index based on mode (train/valid/test)\n",
    "        if self.mode == \"valid\":\n",
    "            index += self.train_end\n",
    "        elif self.mode == \"test\":\n",
    "            index += self.valid_end\n",
    "\n",
    "        zetas, velocities = [], []\n",
    "\n",
    "        # Read data for the given index from the HDF5 file\n",
    "        with h5py.File(self.file_path, \"r\") as hdf_file:\n",
    "            for i in range(self.order):\n",
    "                # Normalize Elevation and velocity using min-max scaling\n",
    "                zeta = hdf_file[f\"timestep_{index + i}\"][\"elevation\"][:]\n",
    "                vel = hdf_file[f\"timestep_{index + i}\"][\"velocity\"][:]\n",
    "                zeta, vel = self.normalize_vals(zeta, vel)\n",
    "                \n",
    "                # Add an extra dimension to match the expected input shape\n",
    "                zetas.append(zeta[None, :])\n",
    "                velocities.append(vel[None, :])\n",
    "\n",
    "            # Extract and normalize target data for the given index\n",
    "            target_zeta = hdf_file[f\"timestep_{index + self.order}\"][\"elevation\"][:]\n",
    "            target_vel = hdf_file[f\"timestep_{index + self.order}\"][\"velocity\"][:]\n",
    "            target_zeta, target_vel = self.normalize_vals(target_zeta, target_vel)\n",
    "\n",
    "        # Concatenate the input data\n",
    "        input_zeta = np.concatenate(zetas, axis=0)\n",
    "        input_vel = np.concatenate(velocities, axis=0)\n",
    "\n",
    "        # Convert input and target data to PyTorch tensors\n",
    "        return (\n",
    "            torch.tensor(input_zeta, dtype=torch.float32),\n",
    "            torch.tensor(input_vel, dtype=torch.float32),\n",
    "        ), (\n",
    "            torch.tensor(\n",
    "                target_zeta[None, :], dtype=torch.float32\n",
    "            ),  # Add an extra dimension\n",
    "            torch.tensor(\n",
    "                target_vel[None, :], dtype=torch.float32\n",
    "            ),  # Add an extra dimension\n",
    "        )\n",
    "\n",
    "    def get_initial_conditions(self):\n",
    "        return self.init_zeta.unsqueeze(0).unsqueeze(0), self.init_vel.unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
