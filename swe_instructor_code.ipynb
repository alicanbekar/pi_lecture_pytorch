{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "* arrange order of material\n",
    "* new task before any training: load some simulation data and visualize a simulation\n",
    "    * image plot\n",
    "    * v/z at a few times\n",
    "* include multiple ICs for supervised training\n",
    "* provide mask as an input channel always (even for unet)\n",
    "* new first task: train without using fixed BCs using UnetBC\n",
    "  * visualize results after training\n",
    "  * image plots v/z ref/est/diff\n",
    "  * plot v/z at some chosen time points (ref/est)\n",
    "  * scatter plot of delta v and delta z ref vs. est (over all locations / time points)\n",
    "* rename updates to delta_zeta, delta_v\n",
    "* visualization code\n",
    "  * visualize numerical simulations: give them an example for plotting a simulation, ask them to do a minor variation on this\n",
    "  * give instructions\n",
    "* flux task\n",
    "  * before implementing flux net, have students graph total mass over time of data-driven BCnet\n",
    "  * change flux output so z fluxes defined on vel points, velocities still as tendencies. keep BC constraint\n",
    "  * standard output visualization for fluxnet\n",
    "  * compare overall error and total mass over time of fluxnet to BCnet\n",
    "* new task: hybrid net with supervised loss\n",
    "  * inputs -> net -> flux -> zeta -> u. keep imposing BCs.\n",
    "  * train on same supervised loss as before\n",
    "  * plots, comparisons, etc.\n",
    "* final task: unsupervised learning\n",
    "  * pde loss from $A\\zeta - b$\n",
    "  * use library of system states, randomly sample from these to generate each batch. restart with new random state when integrating past $t_\\text{max}$.\n",
    "* remove order parameter (multiple inputs time steps)?\n",
    "* extra tasks: change hyperparams, multiple random seeds\n",
    "* link to papers: unet, \n",
    "* further reading\n",
    "* student version\n",
    "* separate cell and markdown explanation of random seeding func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SW Equations and Custom Dataset for SW Simulation Data\n",
    "\n",
    "We are interested in integrating the following form of Shallow Water Equations\n",
    "\n",
    "$\\dfrac{\\partial v}{\\partial t} + \\dfrac{c_D}{h}v|v|+g\\dfrac{\\partial \\zeta}{\\partial x} = 0$\n",
    "\n",
    "$\\dfrac{\\partial \\zeta}{\\partial t} + \\dfrac{\\partial (vh)}{\\partial x}=0$\n",
    "\n",
    "where $u(x, t)$ is the velocity, $ζ(x, t)$ is the positive or negative surface disturbance and $h(x, t) = \\zeta(x, t) + h_0$ is the total depth. $c_D$ and $g$ correspond to drag coefficient and graviational acceleration respectively. The first equation describes Newton's second law ($F=ma$) acting on a fluid parcel, while the second models mass conservation.\n",
    "\n",
    "We'll discuss the discretization and time stepping used to solve the PDE in one of our later tasks. For now, it's enough to know that we've generated some simulation data from numerical simulation code, and we'll use that data to train neural networks and as a \"ground truth\" reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with the Dataset\n",
    "\n",
    "In this section, we'll import and use custom PyTorch class to load SWE simulation data from an HDF5 file. If you'd like to see how this class works later you can read through the code [here](https://github.com/alicanbekar/pi_lecture_pytorch/blob/main/sw_dataset.ipynb), but for now that isn't necessary. This dataset class is responsible for:\n",
    "1. Reading data from an HDF5 file.\n",
    "2. Normalizing the data, so that $\\zeta$ and $v$ both range from roughly -1 to 1.\n",
    "3. Splitting the data into training, validation, and testing datasets.\n",
    "4. Collecting batches of SWE system state sequences, and streaming these to the training, validation and testing routines.\n",
    "\n",
    "Run the next cell to install the necessary code and download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -outube-dl (/home/greenber/miniconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -outube-dl (/home/greenber/miniconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -outube-dl (/home/greenber/miniconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -outube-dl (/home/greenber/miniconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -outube-dl (/home/greenber/miniconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -outube-dl (/home/greenber/miniconda3/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m--2023-09-19 11:14:37--  https://raw.githubusercontent.com/alicanbekar/pi_lecture_pytorch/main/sw_dataset.ipynb\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7941 (7.8K) [text/plain]\n",
      "Saving to: ‘sw_dataset.ipynb.3’\n",
      "\n",
      "sw_dataset.ipynb.3  100%[===================>]   7.75K  --.-KB/s    in 0.004s  \n",
      "\n",
      "2023-09-19 11:14:37 (2.02 MB/s) - ‘sw_dataset.ipynb.3’ saved [7941/7941]\n",
      "\n",
      "--2023-09-19 11:14:37--  http://!wget/\n",
      "Resolving !wget (!wget)... failed: Name or service not known.\n",
      "wget: unable to resolve host address ‘!wget’\n",
      "--2023-09-19 11:14:37--  https://raw.githubusercontent.com/alicanbekar/pi_lecture_pytorch/main/simulation_data.h5\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6044412 (5.8M) [application/octet-stream]\n",
      "Saving to: ‘simulation_data.h5.3’\n",
      "\n",
      "simulation_data.h5. 100%[===================>]   5.76M  3.63MB/s    in 1.6s    \n",
      "\n",
      "2023-09-19 11:14:39 (3.63 MB/s) - ‘simulation_data.h5.3’ saved [6044412/6044412]\n",
      "\n",
      "FINISHED --2023-09-19 11:14:39--\n",
      "Total wall clock time: 1.7s\n",
      "Downloaded: 1 files, 5.8M in 1.6s (3.63 MB/s)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q import-ipynb\n",
    "import import_ipynb\n",
    "!wget https://raw.githubusercontent.com/alicanbekar/pi_lecture_pytorch/main/sw_dataset.ipynb\n",
    "!wget !wget https://raw.githubusercontent.com/alicanbekar/pi_lecture_pytorch/main/simulation_data.h5\n",
    "%run sw_dataset.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the U-Net Model for SW Simulations\n",
    "\n",
    "U-Net is a convolutional neural network architecture primarily used for biomedical image segmentation. In our case, we will adapt U-Net to handle 1D data from the SW simulations.\n",
    "\n",
    "The U-Net architecture is symmetric, and it consists of an encoding (downsampling) path, followed by a decoding (upsampling) path. Skip connections are used to pass the information from the encoding path to the decoding path, which helps the network retain spatial details.\n",
    "\n",
    "Let's walk through the code and its structure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, order):\n",
    "        super(UNet, self).__init__()\n",
    "        self.order = order\n",
    "\n",
    "        # Initial convolution layers for two different input types\n",
    "        self.conv_zeta = nn.Conv1d(self.order, 8, kernel_size=3, padding=1)\n",
    "        self.conv_vel = nn.Conv1d(self.order, 8, kernel_size=4, padding=1)\n",
    "\n",
    "        # Encoder (downsampling) blocks\n",
    "        self.enc1 = self.u_net_block(16, 16)\n",
    "        self.enc2 = self.u_net_block(16, 32)\n",
    "        self.enc3 = self.u_net_block(32, 64)\n",
    "        self.enc4 = self.u_net_block(64, 128)\n",
    "        self.enc5 = self.u_net_block(128, 256)\n",
    "\n",
    "        # Pooling layer for downsampling\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "\n",
    "        # Upsampling layers\n",
    "        self.up1 = nn.ConvTranspose1d(256, 128, kernel_size=2, stride=2)\n",
    "        self.up2 = nn.ConvTranspose1d(128, 64, kernel_size=2, stride=2)\n",
    "        self.up3 = nn.ConvTranspose1d(64, 32, kernel_size=2, stride=2)\n",
    "        self.up4 = nn.ConvTranspose1d(32, 16, kernel_size=2, stride=2)\n",
    "\n",
    "        # Decoder (upsampling) blocks\n",
    "        self.dec1 = self.u_net_block(256, 128)\n",
    "        self.dec2 = self.u_net_block(128, 64)\n",
    "        self.dec3 = self.u_net_block(64, 32)\n",
    "        self.dec4 = self.u_net_block(32, 16)\n",
    "\n",
    "        # Output convolution layers\n",
    "        self.output_dec_zeta = nn.Conv1d(16, 1, kernel_size=3, padding=1)\n",
    "        self.output_dec_vel = nn.Conv1d(16, 1, kernel_size=2, padding=1)\n",
    "\n",
    "    def u_net_block(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Creates a U-Net block with two convolution layers followed by batch normalization and ReLU activation.\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_zeta, x_vel):\n",
    "        # Initial convolution operations\n",
    "        x_zeta = self.conv_zeta(x_zeta)\n",
    "        x_vel = self.conv_vel(x_vel)\n",
    "\n",
    "        # Concatenate the two feature maps along the channel dimension\n",
    "        x_combined = torch.cat([x_vel, x_zeta], dim=1)\n",
    "\n",
    "        # Encoding process\n",
    "        e1 = self.enc1(x_combined)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.enc4(self.pool(e3))\n",
    "        e5 = self.enc5(self.pool(e4))\n",
    "\n",
    "        # Decoding process with skip connections\n",
    "        d1 = self.up1(e5)\n",
    "        d1 = torch.cat([d1, e4], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "\n",
    "        d2 = self.up2(d1)\n",
    "        d2 = torch.cat([d2, e3], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "\n",
    "        d3 = self.up3(d2)\n",
    "        d3 = torch.cat([d3, e2], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "\n",
    "        d4 = self.up4(d3)\n",
    "        d4 = torch.cat([d4, e1], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "\n",
    "        # Separate output convolutions\n",
    "        dt_zeta = self.output_dec_zeta(d4)\n",
    "        dt_vel = self.output_dec_vel(d4)\n",
    "        return dt_zeta, dt_vel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines:\n",
    "\n",
    "- **u_net_block**: This function should return a sequential block that performs two sets of (convolution -> batch normalization -> ReLU activation). You can chain these operations using `nn.Sequential`.\n",
    "\n",
    "- **Encoder**: Remember, as you go deeper into the encoder, you are reducing the spatial dimensions (using max pooling) and typically increasing the number of channels.\n",
    "\n",
    "- **Decoder**: It's the reverse of the encoder. For each block, you will upsample to increase spatial dimensions and typically decrease the number of channels. Make sure to include the skip connections from the encoder. This can be done using torch's concatenation.\n",
    "\n",
    "- **Output Layers**: The goal is to transform the deep feature maps into our desired output. Depending on the task, this could be a segmentation mask, regression map, etc.\n",
    "\n",
    "Remember, the architecture of U-Net is symmetric. It might be helpful to sketch the network or list down the sizes of feature maps as you code.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boundary Conditions can be applied using a boundary mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetBC(UNet):\n",
    "    def __init__(self, order, mask):\n",
    "        super(UNetBC, self).__init__(order)\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, x_zeta, x_vel):\n",
    "        x_vel = x_vel * self.mask\n",
    "        dt_zeta, dt_vel = super(UNetBC, self).forward(x_zeta, x_vel)\n",
    "        dt_vel = dt_vel * self.mask\n",
    "        return dt_zeta, dt_vel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the U-Net Model\n",
    "\n",
    "After defining our U-Net architecture, it's time to set up a training loop. This loop will iteratively update our model's weights using our dataset. Let's break down the steps needed:\n",
    "\n",
    "1. **Setting Up**: Import necessary libraries, define hyperparameters, initialize computational device, and set random seeds.\n",
    "2. **Data Loading**: Load the training and validation datasets and create data loaders.\n",
    "3. **Model & Training Essentials Initialization**: Create mask, model, optimizer, and loss function.\n",
    "4. **Training Loop**: For each epoch, forward propagate the input through the model, compute the loss, backpropagate the errors, and update the model weights.\n",
    "5. **Validation Loop**: After training for each epoch, we will evaluate the model's performance on the validation dataset.\n",
    "6. **Logging & Visualization**: Log metrics such as losses to TensorBoard.\n",
    "7. **Model Saving**: After all epochs are completed, save the model's state dict.\n",
    "\n",
    "Let's get started with the skeleton and explanations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setting Up\n",
    "\n",
    "First, we need to define some hyperparameters, which are constants that determine how the model will be trained. We also set a computational device (either a GPU or CPU) to ensure our tensors and model are loaded onto the right hardware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 400\n",
    "LR = 0.001\n",
    "ORDER = 1  # Autoregressive model order\n",
    "NUMTIME = 600  # time steps per simulation\n",
    "EXP_NAME = 'plain_time_integrator'\n",
    "\n",
    "# Initialize TensorBoard writer for logging\n",
    "log_dir = f'runs/exp_{EXP_NAME}'\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    # This function ensures reproducibility\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Loading\n",
    "\n",
    "Next, load the training and validation datasets. We also create data loaders that will allow us to fetch batches of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SWDataset(file_path=\"simulation_data.h5\", order=ORDER, numtime=NUMTIME, mode=\"train\")\n",
    "valid_dataset = SWDataset(file_path=\"simulation_data.h5\", order=ORDER, numtime=NUMTIME, mode=\"valid\")\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "dataloader_valid = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Model & Training Essentials Initialization\n",
    "\n",
    "Before training, initialize the model, the optimizer responsible for weight updates, and the loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader_train))\n",
    "input_vel_batch = batch[0][1]\n",
    "mask = torch.ones_like(input_vel_batch).to(device)\n",
    "mask[..., 0] = 0\n",
    "mask[..., -1] = 0\n",
    "\n",
    "model = UNetBC(order=ORDER, mask=mask).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 & 5: Training and Validation Loop\n",
    "\n",
    "The training loop involves:\n",
    "1. Setting the model to training mode.\n",
    "2. Iterating through batches of data from the dataloader.\n",
    "3. Making predictions using the model.\n",
    "4. Calculating the loss.\n",
    "5. Backpropagating to compute gradients.\n",
    "6. Updating model parameters using the optimizer.\n",
    "\n",
    "After each training epoch, you'll also run a validation loop to check the model's performance on unseen data. We will develop this training loop as a class which can take our UNet model as input, so when we make modifications to the architecture, we can still use this trainer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400 Train Loss: 0.00005304 Valid Loss: 0.00004015\n",
      "Epoch 2/400 Train Loss: 0.00005268 Valid Loss: 0.00004229\n",
      "Epoch 3/400 Train Loss: 0.00004983 Valid Loss: 0.00004286\n",
      "Epoch 4/400 Train Loss: 0.00004696 Valid Loss: 0.00004000\n",
      "Epoch 5/400 Train Loss: 0.00004627 Valid Loss: 0.00003765\n",
      "Epoch 6/400 Train Loss: 0.00004567 Valid Loss: 0.00003784\n",
      "Epoch 7/400 Train Loss: 0.00004445 Valid Loss: 0.00003554\n",
      "Epoch 8/400 Train Loss: 0.00004203 Valid Loss: 0.00003473\n",
      "Epoch 9/400 Train Loss: 0.00004203 Valid Loss: 0.00003453\n",
      "Epoch 10/400 Train Loss: 0.00004062 Valid Loss: 0.00003369\n",
      "Epoch 11/400 Train Loss: 0.00003913 Valid Loss: 0.00003778\n",
      "Epoch 12/400 Train Loss: 0.00004470 Valid Loss: 0.00003914\n",
      "Epoch 13/400 Train Loss: 0.00004356 Valid Loss: 0.00003553\n",
      "Epoch 14/400 Train Loss: 0.00004155 Valid Loss: 0.00003251\n",
      "Epoch 15/400 Train Loss: 0.00003717 Valid Loss: 0.00003404\n",
      "Epoch 16/400 Train Loss: 0.00003924 Valid Loss: 0.00003399\n",
      "Epoch 17/400 Train Loss: 0.00003865 Valid Loss: 0.00003038\n",
      "Epoch 18/400 Train Loss: 0.00003423 Valid Loss: 0.00003117\n",
      "Epoch 19/400 Train Loss: 0.00003476 Valid Loss: 0.00003346\n",
      "Epoch 20/400 Train Loss: 0.00003689 Valid Loss: 0.00003020\n",
      "Epoch 21/400 Train Loss: 0.00003863 Valid Loss: 0.00002956\n",
      "Epoch 22/400 Train Loss: 0.00003401 Valid Loss: 0.00002833\n",
      "Epoch 23/400 Train Loss: 0.00003198 Valid Loss: 0.00002740\n",
      "Epoch 24/400 Train Loss: 0.00003168 Valid Loss: 0.00002872\n",
      "Epoch 25/400 Train Loss: 0.00003014 Valid Loss: 0.00003056\n",
      "Epoch 26/400 Train Loss: 0.00002885 Valid Loss: 0.00002642\n",
      "Epoch 27/400 Train Loss: 0.00003222 Valid Loss: 0.00002760\n",
      "Epoch 28/400 Train Loss: 0.00003302 Valid Loss: 0.00002799\n",
      "Epoch 29/400 Train Loss: 0.00003086 Valid Loss: 0.00002690\n",
      "Epoch 30/400 Train Loss: 0.00003241 Valid Loss: 0.00003304\n",
      "Epoch 31/400 Train Loss: 0.00002797 Valid Loss: 0.00002577\n",
      "Epoch 32/400 Train Loss: 0.00002608 Valid Loss: 0.00002372\n",
      "Epoch 33/400 Train Loss: 0.00002675 Valid Loss: 0.00002390\n",
      "Epoch 34/400 Train Loss: 0.00002547 Valid Loss: 0.00002328\n",
      "Epoch 35/400 Train Loss: 0.00002746 Valid Loss: 0.00002200\n",
      "Epoch 36/400 Train Loss: 0.00002667 Valid Loss: 0.00002475\n",
      "Epoch 37/400 Train Loss: 0.00002938 Valid Loss: 0.00002863\n",
      "Epoch 38/400 Train Loss: 0.00002521 Valid Loss: 0.00002206\n",
      "Epoch 39/400 Train Loss: 0.00002887 Valid Loss: 0.00002434\n",
      "Epoch 40/400 Train Loss: 0.00002929 Valid Loss: 0.00002762\n",
      "Epoch 41/400 Train Loss: 0.00002519 Valid Loss: 0.00002577\n",
      "Epoch 42/400 Train Loss: 0.00002346 Valid Loss: 0.00002243\n",
      "Epoch 43/400 Train Loss: 0.00002469 Valid Loss: 0.00002308\n",
      "Epoch 44/400 Train Loss: 0.00002026 Valid Loss: 0.00002001\n",
      "Epoch 45/400 Train Loss: 0.00002185 Valid Loss: 0.00002102\n",
      "Epoch 46/400 Train Loss: 0.00002051 Valid Loss: 0.00001943\n",
      "Epoch 47/400 Train Loss: 0.00001967 Valid Loss: 0.00002043\n",
      "Epoch 48/400 Train Loss: 0.00001912 Valid Loss: 0.00001958\n",
      "Epoch 49/400 Train Loss: 0.00001981 Valid Loss: 0.00001938\n",
      "Epoch 50/400 Train Loss: 0.00002090 Valid Loss: 0.00002290\n",
      "Epoch 51/400 Train Loss: 0.00001943 Valid Loss: 0.00002312\n",
      "Epoch 52/400 Train Loss: 0.00002448 Valid Loss: 0.00002326\n",
      "Epoch 53/400 Train Loss: 0.00002553 Valid Loss: 0.00002312\n",
      "Epoch 54/400 Train Loss: 0.00002424 Valid Loss: 0.00002021\n",
      "Epoch 55/400 Train Loss: 0.00002372 Valid Loss: 0.00002519\n",
      "Epoch 56/400 Train Loss: 0.00002392 Valid Loss: 0.00002540\n",
      "Epoch 57/400 Train Loss: 0.00002098 Valid Loss: 0.00002041\n",
      "Epoch 58/400 Train Loss: 0.00001966 Valid Loss: 0.00002053\n",
      "Epoch 59/400 Train Loss: 0.00001989 Valid Loss: 0.00001892\n",
      "Epoch 60/400 Train Loss: 0.00001780 Valid Loss: 0.00002081\n",
      "Epoch 61/400 Train Loss: 0.00001842 Valid Loss: 0.00001653\n",
      "Epoch 62/400 Train Loss: 0.00002064 Valid Loss: 0.00002252\n",
      "Epoch 63/400 Train Loss: 0.00001839 Valid Loss: 0.00001681\n",
      "Epoch 64/400 Train Loss: 0.00001753 Valid Loss: 0.00001757\n",
      "Epoch 65/400 Train Loss: 0.00001685 Valid Loss: 0.00001782\n",
      "Epoch 66/400 Train Loss: 0.00001595 Valid Loss: 0.00001752\n",
      "Epoch 67/400 Train Loss: 0.00001565 Valid Loss: 0.00001736\n",
      "Epoch 68/400 Train Loss: 0.00001824 Valid Loss: 0.00001754\n",
      "Epoch 69/400 Train Loss: 0.00001813 Valid Loss: 0.00002033\n",
      "Epoch 70/400 Train Loss: 0.00001836 Valid Loss: 0.00001726\n",
      "Epoch 71/400 Train Loss: 0.00001610 Valid Loss: 0.00001653\n",
      "Epoch 72/400 Train Loss: 0.00001722 Valid Loss: 0.00001640\n",
      "Epoch 73/400 Train Loss: 0.00001567 Valid Loss: 0.00001839\n",
      "Epoch 74/400 Train Loss: 0.00001780 Valid Loss: 0.00001660\n",
      "Epoch 75/400 Train Loss: 0.00001406 Valid Loss: 0.00001705\n",
      "Epoch 76/400 Train Loss: 0.00001790 Valid Loss: 0.00001731\n",
      "Epoch 77/400 Train Loss: 0.00001503 Valid Loss: 0.00001624\n",
      "Epoch 78/400 Train Loss: 0.00001441 Valid Loss: 0.00001566\n",
      "Epoch 79/400 Train Loss: 0.00001510 Valid Loss: 0.00001871\n",
      "Epoch 80/400 Train Loss: 0.00001661 Valid Loss: 0.00001764\n",
      "Epoch 81/400 Train Loss: 0.00001538 Valid Loss: 0.00001488\n",
      "Epoch 82/400 Train Loss: 0.00001331 Valid Loss: 0.00001444\n",
      "Epoch 83/400 Train Loss: 0.00001302 Valid Loss: 0.00001493\n",
      "Epoch 84/400 Train Loss: 0.00001409 Valid Loss: 0.00001456\n",
      "Epoch 85/400 Train Loss: 0.00001422 Valid Loss: 0.00001581\n",
      "Epoch 86/400 Train Loss: 0.00001371 Valid Loss: 0.00001529\n",
      "Epoch 87/400 Train Loss: 0.00001400 Valid Loss: 0.00001869\n",
      "Epoch 88/400 Train Loss: 0.00001426 Valid Loss: 0.00001749\n",
      "Epoch 89/400 Train Loss: 0.00001714 Valid Loss: 0.00001472\n",
      "Epoch 90/400 Train Loss: 0.00001647 Valid Loss: 0.00001801\n",
      "Epoch 91/400 Train Loss: 0.00002220 Valid Loss: 0.00002456\n",
      "Epoch 92/400 Train Loss: 0.00001737 Valid Loss: 0.00001643\n",
      "Epoch 93/400 Train Loss: 0.00001464 Valid Loss: 0.00001316\n",
      "Epoch 94/400 Train Loss: 0.00001126 Valid Loss: 0.00001541\n",
      "Epoch 95/400 Train Loss: 0.00001234 Valid Loss: 0.00001394\n",
      "Epoch 96/400 Train Loss: 0.00001270 Valid Loss: 0.00001487\n",
      "Epoch 97/400 Train Loss: 0.00001351 Valid Loss: 0.00001512\n",
      "Epoch 98/400 Train Loss: 0.00001343 Valid Loss: 0.00001503\n",
      "Epoch 99/400 Train Loss: 0.00001214 Valid Loss: 0.00001437\n",
      "Epoch 100/400 Train Loss: 0.00001442 Valid Loss: 0.00001540\n",
      "Epoch 101/400 Train Loss: 0.00001566 Valid Loss: 0.00001676\n",
      "Epoch 102/400 Train Loss: 0.00001536 Valid Loss: 0.00002039\n",
      "Epoch 103/400 Train Loss: 0.00001570 Valid Loss: 0.00001679\n",
      "Epoch 104/400 Train Loss: 0.00001246 Valid Loss: 0.00001639\n",
      "Epoch 105/400 Train Loss: 0.00001217 Valid Loss: 0.00001232\n",
      "Epoch 106/400 Train Loss: 0.00001168 Valid Loss: 0.00001532\n",
      "Epoch 107/400 Train Loss: 0.00001171 Valid Loss: 0.00001421\n",
      "Epoch 108/400 Train Loss: 0.00001057 Valid Loss: 0.00001236\n",
      "Epoch 109/400 Train Loss: 0.00001320 Valid Loss: 0.00001527\n",
      "Epoch 110/400 Train Loss: 0.00001313 Valid Loss: 0.00001363\n",
      "Epoch 111/400 Train Loss: 0.00001368 Valid Loss: 0.00001895\n",
      "Epoch 112/400 Train Loss: 0.00001342 Valid Loss: 0.00001426\n",
      "Epoch 113/400 Train Loss: 0.00001212 Valid Loss: 0.00001517\n",
      "Epoch 114/400 Train Loss: 0.00001037 Valid Loss: 0.00001215\n",
      "Epoch 115/400 Train Loss: 0.00001258 Valid Loss: 0.00001476\n",
      "Epoch 116/400 Train Loss: 0.00001163 Valid Loss: 0.00001494\n",
      "Epoch 117/400 Train Loss: 0.00001156 Valid Loss: 0.00001357\n",
      "Epoch 118/400 Train Loss: 0.00001250 Valid Loss: 0.00001728\n",
      "Epoch 119/400 Train Loss: 0.00001274 Valid Loss: 0.00001358\n",
      "Epoch 120/400 Train Loss: 0.00001168 Valid Loss: 0.00001208\n",
      "Epoch 121/400 Train Loss: 0.00001227 Valid Loss: 0.00001821\n",
      "Epoch 122/400 Train Loss: 0.00001529 Valid Loss: 0.00001576\n",
      "Epoch 123/400 Train Loss: 0.00001404 Valid Loss: 0.00001164\n",
      "Epoch 124/400 Train Loss: 0.00001075 Valid Loss: 0.00001273\n",
      "Epoch 125/400 Train Loss: 0.00000933 Valid Loss: 0.00001200\n",
      "Epoch 126/400 Train Loss: 0.00001191 Valid Loss: 0.00001554\n",
      "Epoch 127/400 Train Loss: 0.00001332 Valid Loss: 0.00001207\n",
      "Epoch 128/400 Train Loss: 0.00000903 Valid Loss: 0.00001169\n",
      "Epoch 129/400 Train Loss: 0.00001016 Valid Loss: 0.00001291\n",
      "Epoch 130/400 Train Loss: 0.00001116 Valid Loss: 0.00001325\n",
      "Epoch 131/400 Train Loss: 0.00001107 Valid Loss: 0.00001221\n",
      "Epoch 132/400 Train Loss: 0.00001155 Valid Loss: 0.00001555\n",
      "Epoch 133/400 Train Loss: 0.00001226 Valid Loss: 0.00001383\n",
      "Epoch 134/400 Train Loss: 0.00001253 Valid Loss: 0.00001373\n",
      "Epoch 135/400 Train Loss: 0.00000982 Valid Loss: 0.00001262\n",
      "Epoch 136/400 Train Loss: 0.00001156 Valid Loss: 0.00001195\n",
      "Epoch 137/400 Train Loss: 0.00001040 Valid Loss: 0.00001174\n",
      "Epoch 138/400 Train Loss: 0.00001022 Valid Loss: 0.00001076\n",
      "Epoch 139/400 Train Loss: 0.00000957 Valid Loss: 0.00001051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/400 Train Loss: 0.00001068 Valid Loss: 0.00001145\n",
      "Epoch 141/400 Train Loss: 0.00000915 Valid Loss: 0.00001201\n",
      "Epoch 142/400 Train Loss: 0.00000948 Valid Loss: 0.00001479\n",
      "Epoch 143/400 Train Loss: 0.00001233 Valid Loss: 0.00001197\n",
      "Epoch 144/400 Train Loss: 0.00001110 Valid Loss: 0.00001132\n",
      "Epoch 145/400 Train Loss: 0.00001153 Valid Loss: 0.00001167\n",
      "Epoch 146/400 Train Loss: 0.00001010 Valid Loss: 0.00001275\n",
      "Epoch 147/400 Train Loss: 0.00000807 Valid Loss: 0.00001027\n",
      "Epoch 148/400 Train Loss: 0.00000789 Valid Loss: 0.00001108\n",
      "Epoch 149/400 Train Loss: 0.00000783 Valid Loss: 0.00001159\n",
      "Epoch 150/400 Train Loss: 0.00001042 Valid Loss: 0.00001058\n",
      "Epoch 151/400 Train Loss: 0.00000896 Valid Loss: 0.00001162\n",
      "Epoch 152/400 Train Loss: 0.00000823 Valid Loss: 0.00001279\n",
      "Epoch 153/400 Train Loss: 0.00000984 Valid Loss: 0.00001228\n",
      "Epoch 154/400 Train Loss: 0.00000982 Valid Loss: 0.00001163\n",
      "Epoch 155/400 Train Loss: 0.00000988 Valid Loss: 0.00001029\n",
      "Epoch 156/400 Train Loss: 0.00000879 Valid Loss: 0.00001208\n",
      "Epoch 157/400 Train Loss: 0.00000914 Valid Loss: 0.00001120\n",
      "Epoch 158/400 Train Loss: 0.00000867 Valid Loss: 0.00001128\n",
      "Epoch 159/400 Train Loss: 0.00000892 Valid Loss: 0.00001212\n",
      "Epoch 160/400 Train Loss: 0.00000802 Valid Loss: 0.00001286\n",
      "Epoch 161/400 Train Loss: 0.00001179 Valid Loss: 0.00001378\n",
      "Epoch 162/400 Train Loss: 0.00001107 Valid Loss: 0.00001377\n",
      "Epoch 163/400 Train Loss: 0.00001007 Valid Loss: 0.00001353\n",
      "Epoch 164/400 Train Loss: 0.00001048 Valid Loss: 0.00001179\n",
      "Epoch 165/400 Train Loss: 0.00000884 Valid Loss: 0.00001120\n",
      "Epoch 166/400 Train Loss: 0.00001010 Valid Loss: 0.00001014\n",
      "Epoch 167/400 Train Loss: 0.00001150 Valid Loss: 0.00001137\n",
      "Epoch 168/400 Train Loss: 0.00000874 Valid Loss: 0.00000953\n",
      "Epoch 169/400 Train Loss: 0.00000763 Valid Loss: 0.00001061\n",
      "Epoch 170/400 Train Loss: 0.00000878 Valid Loss: 0.00001085\n",
      "Epoch 171/400 Train Loss: 0.00000752 Valid Loss: 0.00001421\n",
      "Epoch 172/400 Train Loss: 0.00000921 Valid Loss: 0.00000901\n",
      "Epoch 173/400 Train Loss: 0.00000691 Valid Loss: 0.00000946\n",
      "Epoch 174/400 Train Loss: 0.00000812 Valid Loss: 0.00001123\n",
      "Epoch 175/400 Train Loss: 0.00001035 Valid Loss: 0.00001193\n",
      "Epoch 176/400 Train Loss: 0.00001160 Valid Loss: 0.00001451\n",
      "Epoch 177/400 Train Loss: 0.00001187 Valid Loss: 0.00001154\n",
      "Epoch 178/400 Train Loss: 0.00001023 Valid Loss: 0.00001017\n",
      "Epoch 179/400 Train Loss: 0.00000729 Valid Loss: 0.00000920\n",
      "Epoch 180/400 Train Loss: 0.00000964 Valid Loss: 0.00001162\n",
      "Epoch 181/400 Train Loss: 0.00000835 Valid Loss: 0.00001072\n",
      "Epoch 182/400 Train Loss: 0.00000784 Valid Loss: 0.00001076\n",
      "Epoch 183/400 Train Loss: 0.00000787 Valid Loss: 0.00001089\n",
      "Epoch 184/400 Train Loss: 0.00000644 Valid Loss: 0.00001177\n",
      "Epoch 185/400 Train Loss: 0.00000954 Valid Loss: 0.00001228\n",
      "Epoch 186/400 Train Loss: 0.00001205 Valid Loss: 0.00001004\n",
      "Epoch 187/400 Train Loss: 0.00001056 Valid Loss: 0.00000899\n",
      "Epoch 188/400 Train Loss: 0.00000823 Valid Loss: 0.00001300\n",
      "Epoch 189/400 Train Loss: 0.00000774 Valid Loss: 0.00000890\n",
      "Epoch 190/400 Train Loss: 0.00000816 Valid Loss: 0.00001150\n",
      "Epoch 191/400 Train Loss: 0.00000886 Valid Loss: 0.00001012\n",
      "Epoch 192/400 Train Loss: 0.00000689 Valid Loss: 0.00000955\n",
      "Epoch 193/400 Train Loss: 0.00000633 Valid Loss: 0.00001071\n",
      "Epoch 194/400 Train Loss: 0.00000714 Valid Loss: 0.00000912\n",
      "Epoch 195/400 Train Loss: 0.00000815 Valid Loss: 0.00001055\n",
      "Epoch 196/400 Train Loss: 0.00000755 Valid Loss: 0.00001153\n",
      "Epoch 197/400 Train Loss: 0.00000871 Valid Loss: 0.00000933\n",
      "Epoch 198/400 Train Loss: 0.00000860 Valid Loss: 0.00001217\n",
      "Epoch 199/400 Train Loss: 0.00000940 Valid Loss: 0.00001073\n",
      "Epoch 200/400 Train Loss: 0.00000923 Valid Loss: 0.00000921\n",
      "Epoch 201/400 Train Loss: 0.00000598 Valid Loss: 0.00000816\n",
      "Epoch 202/400 Train Loss: 0.00000650 Valid Loss: 0.00000923\n",
      "Epoch 203/400 Train Loss: 0.00000693 Valid Loss: 0.00000908\n",
      "Epoch 204/400 Train Loss: 0.00000681 Valid Loss: 0.00000976\n",
      "Epoch 205/400 Train Loss: 0.00000591 Valid Loss: 0.00000862\n",
      "Epoch 206/400 Train Loss: 0.00000590 Valid Loss: 0.00000767\n",
      "Epoch 207/400 Train Loss: 0.00000586 Valid Loss: 0.00000757\n",
      "Epoch 208/400 Train Loss: 0.00000581 Valid Loss: 0.00000760\n",
      "Epoch 209/400 Train Loss: 0.00000701 Valid Loss: 0.00000943\n",
      "Epoch 210/400 Train Loss: 0.00000731 Valid Loss: 0.00000811\n",
      "Epoch 211/400 Train Loss: 0.00000590 Valid Loss: 0.00000835\n",
      "Epoch 212/400 Train Loss: 0.00000675 Valid Loss: 0.00001061\n",
      "Epoch 213/400 Train Loss: 0.00000802 Valid Loss: 0.00001046\n",
      "Epoch 214/400 Train Loss: 0.00000794 Valid Loss: 0.00000999\n",
      "Epoch 215/400 Train Loss: 0.00000738 Valid Loss: 0.00000966\n",
      "Epoch 216/400 Train Loss: 0.00000813 Valid Loss: 0.00000966\n",
      "Epoch 217/400 Train Loss: 0.00000843 Valid Loss: 0.00001113\n",
      "Epoch 218/400 Train Loss: 0.00000922 Valid Loss: 0.00001520\n",
      "Epoch 219/400 Train Loss: 0.00000904 Valid Loss: 0.00001370\n",
      "Epoch 220/400 Train Loss: 0.00000991 Valid Loss: 0.00001442\n",
      "Epoch 221/400 Train Loss: 0.00000711 Valid Loss: 0.00000821\n",
      "Epoch 222/400 Train Loss: 0.00000713 Valid Loss: 0.00000914\n",
      "Epoch 223/400 Train Loss: 0.00000605 Valid Loss: 0.00001000\n",
      "Epoch 224/400 Train Loss: 0.00000806 Valid Loss: 0.00001047\n",
      "Epoch 225/400 Train Loss: 0.00000771 Valid Loss: 0.00001052\n",
      "Epoch 226/400 Train Loss: 0.00000819 Valid Loss: 0.00001469\n",
      "Epoch 227/400 Train Loss: 0.00000949 Valid Loss: 0.00001166\n",
      "Epoch 228/400 Train Loss: 0.00000884 Valid Loss: 0.00001031\n",
      "Epoch 229/400 Train Loss: 0.00001043 Valid Loss: 0.00001145\n",
      "Epoch 230/400 Train Loss: 0.00001101 Valid Loss: 0.00000880\n",
      "Epoch 231/400 Train Loss: 0.00001430 Valid Loss: 0.00000913\n",
      "Epoch 232/400 Train Loss: 0.00001399 Valid Loss: 0.00001288\n",
      "Epoch 233/400 Train Loss: 0.00001077 Valid Loss: 0.00001090\n",
      "Epoch 234/400 Train Loss: 0.00001250 Valid Loss: 0.00001080\n",
      "Epoch 235/400 Train Loss: 0.00000842 Valid Loss: 0.00001324\n",
      "Epoch 236/400 Train Loss: 0.00000730 Valid Loss: 0.00000866\n",
      "Epoch 237/400 Train Loss: 0.00000687 Valid Loss: 0.00000796\n",
      "Epoch 238/400 Train Loss: 0.00000679 Valid Loss: 0.00001144\n",
      "Epoch 239/400 Train Loss: 0.00000734 Valid Loss: 0.00000913\n",
      "Epoch 240/400 Train Loss: 0.00000734 Valid Loss: 0.00001157\n",
      "Epoch 241/400 Train Loss: 0.00000694 Valid Loss: 0.00000977\n",
      "Epoch 242/400 Train Loss: 0.00000598 Valid Loss: 0.00000855\n",
      "Epoch 243/400 Train Loss: 0.00000596 Valid Loss: 0.00000767\n",
      "Epoch 244/400 Train Loss: 0.00000528 Valid Loss: 0.00000693\n",
      "Epoch 245/400 Train Loss: 0.00000633 Valid Loss: 0.00000695\n",
      "Epoch 246/400 Train Loss: 0.00000502 Valid Loss: 0.00000765\n",
      "Epoch 247/400 Train Loss: 0.00000549 Valid Loss: 0.00000808\n",
      "Epoch 248/400 Train Loss: 0.00000496 Valid Loss: 0.00000688\n",
      "Epoch 249/400 Train Loss: 0.00000508 Valid Loss: 0.00000873\n",
      "Epoch 250/400 Train Loss: 0.00000591 Valid Loss: 0.00000795\n",
      "Epoch 251/400 Train Loss: 0.00000603 Valid Loss: 0.00000889\n",
      "Epoch 252/400 Train Loss: 0.00000674 Valid Loss: 0.00000837\n",
      "Epoch 253/400 Train Loss: 0.00000746 Valid Loss: 0.00000754\n",
      "Epoch 254/400 Train Loss: 0.00000787 Valid Loss: 0.00001176\n",
      "Epoch 255/400 Train Loss: 0.00001018 Valid Loss: 0.00001485\n",
      "Epoch 256/400 Train Loss: 0.00000933 Valid Loss: 0.00001344\n",
      "Epoch 257/400 Train Loss: 0.00000833 Valid Loss: 0.00000901\n",
      "Epoch 258/400 Train Loss: 0.00000817 Valid Loss: 0.00001115\n",
      "Epoch 259/400 Train Loss: 0.00000592 Valid Loss: 0.00000693\n",
      "Epoch 260/400 Train Loss: 0.00000728 Valid Loss: 0.00000757\n",
      "Epoch 261/400 Train Loss: 0.00000561 Valid Loss: 0.00000640\n",
      "Epoch 262/400 Train Loss: 0.00000425 Valid Loss: 0.00000751\n",
      "Epoch 263/400 Train Loss: 0.00000729 Valid Loss: 0.00000781\n",
      "Epoch 264/400 Train Loss: 0.00000639 Valid Loss: 0.00001267\n",
      "Epoch 265/400 Train Loss: 0.00000734 Valid Loss: 0.00001051\n",
      "Epoch 266/400 Train Loss: 0.00000593 Valid Loss: 0.00000774\n",
      "Epoch 267/400 Train Loss: 0.00000858 Valid Loss: 0.00000770\n",
      "Epoch 268/400 Train Loss: 0.00000640 Valid Loss: 0.00001037\n",
      "Epoch 269/400 Train Loss: 0.00000620 Valid Loss: 0.00000634\n",
      "Epoch 270/400 Train Loss: 0.00000869 Valid Loss: 0.00001096\n",
      "Epoch 271/400 Train Loss: 0.00000800 Valid Loss: 0.00001082\n",
      "Epoch 272/400 Train Loss: 0.00000520 Valid Loss: 0.00000773\n",
      "Epoch 273/400 Train Loss: 0.00000577 Valid Loss: 0.00001543\n",
      "Epoch 274/400 Train Loss: 0.00001019 Valid Loss: 0.00001120\n",
      "Epoch 275/400 Train Loss: 0.00000763 Valid Loss: 0.00001224\n",
      "Epoch 276/400 Train Loss: 0.00000894 Valid Loss: 0.00001155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277/400 Train Loss: 0.00000886 Valid Loss: 0.00001002\n",
      "Epoch 278/400 Train Loss: 0.00001068 Valid Loss: 0.00001304\n",
      "Epoch 279/400 Train Loss: 0.00000714 Valid Loss: 0.00001033\n",
      "Epoch 280/400 Train Loss: 0.00000541 Valid Loss: 0.00000683\n",
      "Epoch 281/400 Train Loss: 0.00000650 Valid Loss: 0.00001235\n",
      "Epoch 282/400 Train Loss: 0.00000732 Valid Loss: 0.00000815\n",
      "Epoch 283/400 Train Loss: 0.00000672 Valid Loss: 0.00001103\n",
      "Epoch 284/400 Train Loss: 0.00000601 Valid Loss: 0.00000827\n",
      "Epoch 285/400 Train Loss: 0.00000655 Valid Loss: 0.00000898\n",
      "Epoch 286/400 Train Loss: 0.00000704 Valid Loss: 0.00000867\n",
      "Epoch 287/400 Train Loss: 0.00000639 Valid Loss: 0.00001170\n",
      "Epoch 288/400 Train Loss: 0.00000712 Valid Loss: 0.00000783\n",
      "Epoch 289/400 Train Loss: 0.00000486 Valid Loss: 0.00000832\n",
      "Epoch 290/400 Train Loss: 0.00000554 Valid Loss: 0.00000911\n",
      "Epoch 291/400 Train Loss: 0.00000822 Valid Loss: 0.00001250\n",
      "Epoch 292/400 Train Loss: 0.00000651 Valid Loss: 0.00000886\n",
      "Epoch 293/400 Train Loss: 0.00000644 Valid Loss: 0.00000780\n",
      "Epoch 294/400 Train Loss: 0.00000627 Valid Loss: 0.00000975\n",
      "Epoch 295/400 Train Loss: 0.00000754 Valid Loss: 0.00001306\n",
      "Epoch 296/400 Train Loss: 0.00000751 Valid Loss: 0.00000776\n",
      "Epoch 297/400 Train Loss: 0.00000758 Valid Loss: 0.00001431\n",
      "Epoch 298/400 Train Loss: 0.00000951 Valid Loss: 0.00000996\n",
      "Epoch 299/400 Train Loss: 0.00000902 Valid Loss: 0.00000727\n",
      "Epoch 300/400 Train Loss: 0.00000545 Valid Loss: 0.00000917\n",
      "Epoch 301/400 Train Loss: 0.00000559 Valid Loss: 0.00000655\n",
      "Epoch 302/400 Train Loss: 0.00000467 Valid Loss: 0.00000777\n",
      "Epoch 303/400 Train Loss: 0.00000451 Valid Loss: 0.00000727\n",
      "Epoch 304/400 Train Loss: 0.00000508 Valid Loss: 0.00000775\n",
      "Epoch 305/400 Train Loss: 0.00000609 Valid Loss: 0.00000719\n",
      "Epoch 306/400 Train Loss: 0.00000511 Valid Loss: 0.00000571\n",
      "Epoch 307/400 Train Loss: 0.00000550 Valid Loss: 0.00000679\n",
      "Epoch 308/400 Train Loss: 0.00000577 Valid Loss: 0.00000696\n",
      "Epoch 309/400 Train Loss: 0.00000728 Valid Loss: 0.00000818\n",
      "Epoch 310/400 Train Loss: 0.00000447 Valid Loss: 0.00000660\n",
      "Epoch 311/400 Train Loss: 0.00000452 Valid Loss: 0.00000773\n",
      "Epoch 312/400 Train Loss: 0.00000565 Valid Loss: 0.00000869\n",
      "Epoch 313/400 Train Loss: 0.00000492 Valid Loss: 0.00000721\n",
      "Epoch 314/400 Train Loss: 0.00000419 Valid Loss: 0.00000625\n",
      "Epoch 315/400 Train Loss: 0.00000429 Valid Loss: 0.00000576\n",
      "Epoch 316/400 Train Loss: 0.00000484 Valid Loss: 0.00000764\n",
      "Epoch 317/400 Train Loss: 0.00000494 Valid Loss: 0.00000761\n",
      "Epoch 318/400 Train Loss: 0.00000701 Valid Loss: 0.00000566\n",
      "Epoch 319/400 Train Loss: 0.00000476 Valid Loss: 0.00000742\n",
      "Epoch 320/400 Train Loss: 0.00000440 Valid Loss: 0.00000523\n",
      "Epoch 321/400 Train Loss: 0.00000360 Valid Loss: 0.00000534\n",
      "Epoch 322/400 Train Loss: 0.00000336 Valid Loss: 0.00000851\n",
      "Epoch 323/400 Train Loss: 0.00000688 Valid Loss: 0.00000800\n",
      "Epoch 324/400 Train Loss: 0.00001121 Valid Loss: 0.00001096\n",
      "Epoch 325/400 Train Loss: 0.00000926 Valid Loss: 0.00001023\n",
      "Epoch 326/400 Train Loss: 0.00000875 Valid Loss: 0.00000746\n",
      "Epoch 327/400 Train Loss: 0.00000628 Valid Loss: 0.00000858\n",
      "Epoch 328/400 Train Loss: 0.00000864 Valid Loss: 0.00001581\n",
      "Epoch 329/400 Train Loss: 0.00000748 Valid Loss: 0.00000791\n",
      "Epoch 330/400 Train Loss: 0.00000569 Valid Loss: 0.00000570\n",
      "Epoch 331/400 Train Loss: 0.00000539 Valid Loss: 0.00000661\n",
      "Epoch 332/400 Train Loss: 0.00000462 Valid Loss: 0.00000707\n",
      "Epoch 333/400 Train Loss: 0.00000663 Valid Loss: 0.00000939\n",
      "Epoch 334/400 Train Loss: 0.00000491 Valid Loss: 0.00000622\n",
      "Epoch 335/400 Train Loss: 0.00000560 Valid Loss: 0.00001091\n",
      "Epoch 336/400 Train Loss: 0.00000659 Valid Loss: 0.00000663\n",
      "Epoch 337/400 Train Loss: 0.00000627 Valid Loss: 0.00000598\n",
      "Epoch 338/400 Train Loss: 0.00000411 Valid Loss: 0.00000582\n",
      "Epoch 339/400 Train Loss: 0.00000494 Valid Loss: 0.00000716\n",
      "Epoch 340/400 Train Loss: 0.00000526 Valid Loss: 0.00000575\n",
      "Epoch 341/400 Train Loss: 0.00000447 Valid Loss: 0.00001080\n",
      "Epoch 342/400 Train Loss: 0.00000689 Valid Loss: 0.00000669\n",
      "Epoch 343/400 Train Loss: 0.00000487 Valid Loss: 0.00000594\n",
      "Epoch 344/400 Train Loss: 0.00000408 Valid Loss: 0.00000749\n",
      "Epoch 345/400 Train Loss: 0.00000495 Valid Loss: 0.00000860\n",
      "Epoch 346/400 Train Loss: 0.00000455 Valid Loss: 0.00000490\n",
      "Epoch 347/400 Train Loss: 0.00000349 Valid Loss: 0.00000512\n",
      "Epoch 348/400 Train Loss: 0.00000307 Valid Loss: 0.00000646\n",
      "Epoch 349/400 Train Loss: 0.00000478 Valid Loss: 0.00000755\n",
      "Epoch 350/400 Train Loss: 0.00000471 Valid Loss: 0.00000621\n",
      "Epoch 351/400 Train Loss: 0.00000505 Valid Loss: 0.00001271\n",
      "Epoch 352/400 Train Loss: 0.00000555 Valid Loss: 0.00000912\n",
      "Epoch 353/400 Train Loss: 0.00000754 Valid Loss: 0.00000769\n",
      "Epoch 354/400 Train Loss: 0.00000579 Valid Loss: 0.00000796\n",
      "Epoch 355/400 Train Loss: 0.00000642 Valid Loss: 0.00000688\n",
      "Epoch 356/400 Train Loss: 0.00000627 Valid Loss: 0.00000562\n",
      "Epoch 357/400 Train Loss: 0.00000623 Valid Loss: 0.00000845\n",
      "Epoch 358/400 Train Loss: 0.00000594 Valid Loss: 0.00000787\n",
      "Epoch 359/400 Train Loss: 0.00000810 Valid Loss: 0.00000819\n",
      "Epoch 360/400 Train Loss: 0.00000838 Valid Loss: 0.00001270\n",
      "Epoch 361/400 Train Loss: 0.00000929 Valid Loss: 0.00000801\n",
      "Epoch 362/400 Train Loss: 0.00000488 Valid Loss: 0.00000995\n",
      "Epoch 363/400 Train Loss: 0.00000849 Valid Loss: 0.00000713\n",
      "Epoch 364/400 Train Loss: 0.00000604 Valid Loss: 0.00000806\n",
      "Epoch 365/400 Train Loss: 0.00000652 Valid Loss: 0.00000943\n",
      "Epoch 366/400 Train Loss: 0.00000549 Valid Loss: 0.00000643\n",
      "Epoch 367/400 Train Loss: 0.00000357 Valid Loss: 0.00000576\n",
      "Epoch 368/400 Train Loss: 0.00000344 Valid Loss: 0.00000682\n",
      "Epoch 369/400 Train Loss: 0.00000380 Valid Loss: 0.00000605\n",
      "Epoch 370/400 Train Loss: 0.00000585 Valid Loss: 0.00000998\n",
      "Epoch 371/400 Train Loss: 0.00000617 Valid Loss: 0.00000731\n",
      "Epoch 372/400 Train Loss: 0.00000665 Valid Loss: 0.00000845\n",
      "Epoch 373/400 Train Loss: 0.00000654 Valid Loss: 0.00000583\n",
      "Epoch 374/400 Train Loss: 0.00000461 Valid Loss: 0.00000486\n",
      "Epoch 375/400 Train Loss: 0.00000345 Valid Loss: 0.00000518\n",
      "Epoch 376/400 Train Loss: 0.00000561 Valid Loss: 0.00001267\n",
      "Epoch 377/400 Train Loss: 0.00000706 Valid Loss: 0.00000762\n",
      "Epoch 378/400 Train Loss: 0.00000516 Valid Loss: 0.00000658\n",
      "Epoch 379/400 Train Loss: 0.00000400 Valid Loss: 0.00000606\n",
      "Epoch 380/400 Train Loss: 0.00000369 Valid Loss: 0.00000742\n",
      "Epoch 381/400 Train Loss: 0.00000394 Valid Loss: 0.00000584\n",
      "Epoch 382/400 Train Loss: 0.00000485 Valid Loss: 0.00000455\n",
      "Epoch 383/400 Train Loss: 0.00000451 Valid Loss: 0.00000486\n",
      "Epoch 384/400 Train Loss: 0.00000325 Valid Loss: 0.00000599\n",
      "Epoch 385/400 Train Loss: 0.00000450 Valid Loss: 0.00000674\n",
      "Epoch 386/400 Train Loss: 0.00000429 Valid Loss: 0.00000698\n",
      "Epoch 387/400 Train Loss: 0.00000616 Valid Loss: 0.00001231\n",
      "Epoch 388/400 Train Loss: 0.00000543 Valid Loss: 0.00000519\n",
      "Epoch 389/400 Train Loss: 0.00000991 Valid Loss: 0.00001344\n",
      "Epoch 390/400 Train Loss: 0.00001043 Valid Loss: 0.00000814\n",
      "Epoch 391/400 Train Loss: 0.00000689 Valid Loss: 0.00000760\n",
      "Epoch 392/400 Train Loss: 0.00000378 Valid Loss: 0.00001040\n",
      "Epoch 393/400 Train Loss: 0.00000726 Valid Loss: 0.00001036\n",
      "Epoch 394/400 Train Loss: 0.00000497 Valid Loss: 0.00000599\n",
      "Epoch 395/400 Train Loss: 0.00000486 Valid Loss: 0.00000718\n",
      "Epoch 396/400 Train Loss: 0.00000452 Valid Loss: 0.00001075\n",
      "Epoch 397/400 Train Loss: 0.00000612 Valid Loss: 0.00000525\n",
      "Epoch 398/400 Train Loss: 0.00000375 Valid Loss: 0.00000535\n",
      "Epoch 399/400 Train Loss: 0.00000509 Valid Loss: 0.00000811\n",
      "Epoch 400/400 Train Loss: 0.00000553 Valid Loss: 0.00000667\n"
     ]
    }
   ],
   "source": [
    "class UNetTrainer:\n",
    "    def __init__(self, model, device, dataloader_train, dataloader_valid, optimizer, criterion, writer, epochs):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.dataloader_train = dataloader_train\n",
    "        self.dataloader_valid = dataloader_valid\n",
    "        self.writer = writer\n",
    "        self.epochs = epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def train_epoch(self):\n",
    "        train_losses = []\n",
    "        self.model.train()\n",
    "\n",
    "        for (input_zeta, input_vel), (target_zeta, target_vel) in self.dataloader_train:\n",
    "            self.optimizer.zero_grad()\n",
    "            input_zeta, input_vel = input_zeta.to(self.device), input_vel.to(self.device)\n",
    "            target_zeta, target_vel = target_zeta.to(self.device), target_vel.to(self.device)\n",
    "            output_zeta_dt, output_vel_dt = self.model(input_zeta, input_vel)\n",
    "            output_zeta = input_zeta + output_zeta_dt\n",
    "            output_vel = input_vel + output_vel_dt\n",
    "            loss_zeta = self.criterion(output_zeta, target_zeta)\n",
    "            loss_vel = self.criterion(output_vel, target_vel)\n",
    "            total_loss = (loss_zeta + loss_vel) / 2.0\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_losses.append(total_loss.item())\n",
    "\n",
    "        return np.mean(train_losses)\n",
    "\n",
    "    def validate_epoch(self):\n",
    "        valid_losses = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for (input_zeta, input_vel), (targets_zeta, targets_vel) in self.dataloader_valid:\n",
    "                input_zeta, input_vel = input_zeta.to(self.device), input_vel.to(self.device)\n",
    "                targets_zeta, targets_vel = targets_zeta.to(self.device), targets_vel.to(self.device)\n",
    "                output_zeta_dt, output_vel_dt = self.model(input_zeta, input_vel)\n",
    "                output_zeta = input_zeta + output_zeta_dt\n",
    "                output_vel = input_vel + output_vel_dt\n",
    "                loss_zeta = self.criterion(output_zeta, targets_zeta)\n",
    "                loss_vel = self.criterion(output_vel, targets_vel)\n",
    "                combined_loss = (loss_zeta + loss_vel) / 2.0\n",
    "                valid_losses.append(combined_loss.item())\n",
    "\n",
    "        return np.mean(valid_losses)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            train_loss = self.train_epoch()\n",
    "            self.writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "\n",
    "            valid_loss = self.validate_epoch()\n",
    "            self.writer.add_scalar(\"Loss/valid\", valid_loss, epoch)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs} Train Loss: {train_loss:.8f} Valid Loss: {valid_loss:.8f}\")\n",
    "\n",
    "trainer = UNetTrainer(model, device, dataloader_train, dataloader_valid, optimizer, criterion, writer, epochs=EPOCHS)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Logging & Visualization\n",
    "\n",
    "We've already added logging functionality in the training loop using TensorBoard's `SummaryWriter`. This will help visualize training and validation loss curves, among other metrics you might want to track.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-716028190b359622\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-716028190b359622\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Model Saving\n",
    "\n",
    "Finally, save the model's state dict, which contains the model's learned parameters. Later, you can load this state dict to make predictions with the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), EXP_NAME + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Learning the flux values instead of tendencies\n",
    "Hyperbolic conservation laws can be written in the form:\n",
    "\n",
    "$\\dfrac{\\partial \\mathbf{U}}{\\partial t} + \\dfrac{\\partial \\mathbf{F}(\\mathbf{U})}{\\partial \\mathbf{x}}=\\mathbf{0}$\n",
    "\n",
    "Instead of outputting the tendencies $\\mathbf{U}_t$ for elevation $\\zeta$ and velocity $v$, we can also output the fluxes $\\mathbf{F}$ calculated on discretized domain corresponding to these variables.\n",
    "This will guarantee that our neural network will satisfy the conservation laws precisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetFlux(UNet):\n",
    "    def __init__(self, order):\n",
    "        super(UNetFlux, self).__init__(order)\n",
    "        self.output_dec_zeta = nn.Conv1d(16, 1, kernel_size=4, padding=1)\n",
    "        self.output_dec_vel = nn.Conv1d(16, 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x_zeta, x_vel):\n",
    "        F_zeta, F_vel = super(UNetFlux, self).forward(x_zeta, x_vel)\n",
    "        zeta_flux = torch.diff(F_zeta, dim=-1)\n",
    "        vel_flux = torch.diff(F_vel, dim=-1)\n",
    "        vel_flux = torch.nn.functional.pad(vel_flux, (1, 1), \"constant\", 0)\n",
    "        zeta_flux = torch.nn.functional.pad(zeta_flux, (1, 1), \"constant\", 0)\n",
    "        return zeta_flux, vel_flux\n",
    "\n",
    "model = UNetFlux(order=1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "trainer = UNetTrainer(model, device, dataloader_train, dataloader_valid, optimizer, criterion, writer, epochs=EPOCHS)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Learning the $\\zeta$ values only using the UNet\n",
    "We can also output updates on $\\zeta$ values and use a hybrid approach to update the field variables. This approach uses the Imex integration scheme for the variables. The derivation of the discretized SWE in this case can be obtained with the following derivation,\n",
    "\n",
    "We discretize the momentum equation as follows:\n",
    "\n",
    "$u^{n+1} = u^n - \\Delta t C_D\\frac{1}{h}u^n|u^n|- \\Delta t g (1-w_{\\textbf{imp}}) \\frac{\\partial \\zeta^{n}}{\\partial x}-\\Delta t g w_{\\textbf{imp}} \\frac{\\partial \\zeta^{n+1}}{\\partial x}$\n",
    "\n",
    "where $w_{\\textbf{imp}}$ is a fixed parameter controlling weighting between implicit and explicit time stepping. The mass equation is discretized as:\n",
    "\n",
    "$\\zeta^{n+1} = \\zeta^n - \\Delta t (1-w_{\\textbf{imp}}) \\frac{\\partial h^n u^n}{\\partial x}-\\Delta t w_{\\textbf{imp}} \\frac{\\partial h^n u^{n+1}}{\\partial x}.\n",
    "$\n",
    "\n",
    "Recall that $h=d+\\zeta$ and $d$ is the undisturbed water depth. Inserting the momentum equation into the mass conservation equation, we obtain:\n",
    "\n",
    "$\\zeta^{n+1} = \\zeta^n - \\Delta t (1-w_{\\textbf{imp}}) \\frac{\\partial h^n u^n}{\\partial x}-\\Delta t w_{\\textbf{imp}} \\frac{\\partial h^nu^*}{\\partial x} + \\Delta t^2 w_{\\textbf{imp}}^2g\\frac{\\partial^2 h^n\\zeta^{n+1}}{\\partial x^2}$\n",
    "\n",
    "where $u^*$ is an explicit prediction for $u$:\n",
    "\n",
    "$u^* = u^n - \\Delta t c_D\\frac{1}{h}u^n|u^n|- \\Delta t g (1 - w_{\\textbf{imp}}) \\frac{\\partial \\zeta^{n}}{\\partial x}$\n",
    "\n",
    "The second order spatial derivatives are discretized using the second order finite central difference stencil. Then using $u^*$, we obtain the following expression for momentum equation:\n",
    "\n",
    "$\\zeta^{n+1}_i = \\frac{1}{1+c_E+c_W}\\bigg[\\zeta^n+\\text{div}+c_E\\zeta^{n+1}_{i+1}+c_W\\zeta^{n+1}_{i-1}\\bigg]$\n",
    "\n",
    "where $\\text{div} = - \\Delta t (1-w_{\\textbf{imp}})\\frac{\\partial h^n u^n}{\\partial x} -\\Delta t w_{\\textbf{imp}}\\frac{\\partial h^nu^*}{\\partial x}$, while $c_E$ and $c_W$ are defined as\n",
    "\n",
    "$c_E=\\frac{0.5\\Delta t^2w_{\\textbf{imp}}^2g (h_i^n+h_{i+1}^n)}{\\Delta x^2}$ if $h(i+1)>0$ and $0$ otherwise\n",
    "\n",
    "$c_W =\\frac{0.5\\Delta t^2w_{\\textbf{imp}}^2g (h_i^n+h_{i-1}^n)}{\\Delta x^2}$ if $h(i-1)>0$ and $0$ otherwise\n",
    "\n",
    "Then $\\zeta$ update equation describes a linear system of equations in $\\zeta^{n+1}$ that can be written in matrix-vector form\n",
    "\n",
    "$A \\zeta^{n+1} = b$\n",
    "\n",
    "where $A$ is a $N \\times N$ tridiagonal matrix ($N=L/\\Delta x$) with $A_{k,k}=1$, $A_{k, k - 1} = -\\frac{c_W}{1 + c_E + c_W}$, $A_{k, k + 1} = - \\frac{c_E}{1 + c_E + c_W}$ and all other elements zero. $b\\in\\mathbb R^N$ with $b = \\frac{\\zeta^n +div}{1 + c_E + c_W}$. Having obtained $\\zeta^{n+1}$, the new velocity $u^{n+1}$ is calculated as\n",
    "\n",
    "$u^{n+1} = u^* - \\Delta t g w_{\\textbf{imp}} \\frac{\\partial \\zeta^{n+1}}{\\partial x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Our tasks are as follows:\n",
    "\n",
    "1- First, modify our UNet architecture to output only one channel.\n",
    "\n",
    "2- Create the loss for updating the $\\zeta$ values using the equation system $A \\zeta^{n+1} = b$.\n",
    "\n",
    "3- Update the velocity values using the formula $v^{n+1} = v^* - \\Delta t g w_{\\textbf{imp}} \\frac{\\partial \\zeta^{n+1}}{\\partial x}$. Hence we need a function accomplishing this.\n",
    "\n",
    "4-  Update the batch size and mask tensor. Call the dataset class for the initial conditions and disable normalization.\n",
    "\n",
    "5- Modify the training loop of our model.\n",
    "\n",
    "6- Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Architecture\n",
    "class ZetaUNet(UNetBC):\n",
    "    def __init__(self, order, mask):\n",
    "        super(ZetaUNet, self).__init__(order, mask)\n",
    "        self.order = order\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, x_zeta, x_vel):\n",
    "        zeta_dt, _ = super(ZetaUNet, self).forward(x_zeta, x_vel)\n",
    "        return zeta_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New physics informed loss function\n",
    "# Parameters for the given dataset of SWE.\n",
    "CD = 1.0e-3\n",
    "G = 9.81\n",
    "DT = 300.0\n",
    "W_IMP = 0.5\n",
    "H0 = 100.0\n",
    "DX = 10.0e3\n",
    "N = 256\n",
    "\n",
    "class HybridLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HybridLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, zeta_dt, zeta_n, vel_n):\n",
    "        h_n = H0 + zeta_n\n",
    "        h_m = (h_n[..., 1:] + h_n[..., :-1]) / 2.0\n",
    "\n",
    "        zeta_dx = torch.diff(zeta_n, axis=-1) / DX\n",
    "        zeta_dx = torch.nn.functional.pad(zeta_dx, (1, 1), \"constant\", 0)\n",
    "        h_m = torch.nn.functional.pad(h_m, (1, 1), \"constant\", 1)\n",
    "        vel_star = (\n",
    "            vel_n\n",
    "            - DT * CD * torch.div(torch.mul(torch.abs(vel_n), vel_n), h_m)\n",
    "            - DT * G * (1 - W_IMP) * zeta_dx\n",
    "            - DT * G * W_IMP * zeta_dx\n",
    "        )\n",
    "        U = torch.mul(vel_n, h_m)\n",
    "        U_star = torch.mul(vel_star, h_m)\n",
    "        U_dx = (torch.diff(U, axis=-1)) / DX\n",
    "        U_star_dx = (torch.diff(U_star, axis=-1)) / DX\n",
    "        div = -DT * (1 - W_IMP) * U_dx - DT * W_IMP * U_star_dx\n",
    "\n",
    "        h_e = h_n + torch.roll(h_n, shifts=-1, dims=-1)\n",
    "        h_w = h_n + torch.roll(h_n, shifts=1, dims=-1)\n",
    "        h_e[..., -1] = 2 * h_n[..., -1]\n",
    "        h_w[..., 0] = 2 * h_n[..., 0]\n",
    "\n",
    "        h_e_dx2 = (h_e[..., :-2] - 2 * h_e[..., 1:-1] + h_e[..., 2:]) / (DX**2)\n",
    "        h_w_dx2 = (h_w[..., :-2] - 2 * h_w[..., 1:-1] + h_w[..., 2:]) / (DX**2)\n",
    "\n",
    "        c_e = DT**2 * W_IMP**2 * G * h_e_dx2\n",
    "        c_w = DT**2 * W_IMP**2 * G * h_w_dx2\n",
    "        diag_c = torch.ones(N)\n",
    "        diag_cm1 = torch.div(-c_w, 1 + c_e + c_w)\n",
    "        diag_cp1 = torch.div(-c_e, 1 + c_e + c_w)\n",
    "        diag_cm1 = torch.nn.functional.pad(diag_cm1, (1, 0), \"constant\", 0)\n",
    "        diag_cp1 = torch.nn.functional.pad(diag_cp1, (0, 1), \"constant\", 0)\n",
    "        b = torch.div(zeta_n[..., 1:-1] + div[..., 1:-1], 1 + c_e + c_w)\n",
    "        b = torch.nn.functional.pad(b, (1, 1), \"constant\", 0)\n",
    "        zeta_new = zeta_n + zeta_dt\n",
    "        A = torch.zeros(BATCH_SIZE, 1, N, N)\n",
    "        A[..., torch.arange(N), torch.arange(N)] = diag_c\n",
    "        A[..., torch.arange(N - 1), torch.arange(1, N)] = diag_cm1\n",
    "        A[..., torch.arange(1, N), torch.arange(N - 1)] = diag_cp1\n",
    "        loss = self.mse_loss(torch.matmul(A, zeta_new.unsqueeze(-1)), b.unsqueeze(-1))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Velocity Integration function\n",
    "def integrate_vel(vel_n, zeta_new, zeta_old):\n",
    "    h_n = H0 + zeta_old\n",
    "    h_m = (h_n[..., 1:] + h_n[..., :-1]) / 2.0\n",
    "\n",
    "    zeta_old_dx = torch.diff(zeta_old, axis=-1) / DX\n",
    "    zeta_new_dx = torch.diff(zeta_new, axis=-1) / DX\n",
    "    zeta_old_dx = torch.nn.functional.pad(zeta_old_dx, (1, 1), \"constant\", 0)\n",
    "    zeta_new_dx = torch.nn.functional.pad(zeta_new_dx, (1, 1), \"constant\", 0)\n",
    "    h_m = torch.nn.functional.pad(h_m, (1, 1), \"constant\", 1)\n",
    "\n",
    "    vel_star = (\n",
    "        vel_n\n",
    "        - DT * CD * torch.div(torch.mul(torch.abs(vel_n), vel_n), h_m)\n",
    "        - DT * G * (1 - W_IMP) * zeta_old_dx\n",
    "        - DT * G * W_IMP * zeta_new_dx\n",
    "    )\n",
    "    vel_new = vel_star - DT * G * W_IMP * zeta_new_dx\n",
    "    return vel_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "train_dataset = SWDataset(file_path=\"simulation_data.h5\", order=ORDER, numtime=NUMTIME, mode=\"train\", normalize=False)\n",
    "valid_dataset = SWDataset(file_path=\"simulation_data.h5\", order=ORDER, numtime=NUMTIME, mode=\"valid\", normalize=False)\n",
    "\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "dataloader_valid = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop for the hybrid model\n",
    "class PITrainerZeta:\n",
    "    def __init__(self, model, device, train_dataset, optimizer, criterion, writer, epochs):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.train_dataset = train_dataset\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.writer = writer\n",
    "        self.EPOCHS = EPOCHS\n",
    "        self.NUMTIME = NUMTIME\n",
    "    def train_epoch(self):\n",
    "        train_losses = []\n",
    "        self.model.train()\n",
    "\n",
    "        init_zeta, init_vel = self.train_dataset.get_initial_conditions()\n",
    "        self.optimizer.zero_grad()\n",
    "        accumulated_loss = 0\n",
    "        input_zeta, input_vel = init_zeta.to(self.device), init_vel.to(self.device)\n",
    "\n",
    "        for _ in range(self.NUMTIME):\n",
    "            output_zeta_dt = self.model(input_zeta, input_vel)\n",
    "            next_zeta = input_zeta + output_zeta_dt\n",
    "            next_vel = integrate_vel(input_vel, next_zeta, input_zeta)\n",
    "\n",
    "            unsupervised_loss = self.criterion(output_zeta_dt, input_zeta, input_vel)\n",
    "            accumulated_loss += unsupervised_loss\n",
    "\n",
    "            input_zeta, input_vel = next_zeta, next_vel\n",
    "\n",
    "        accumulated_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        train_losses.append(accumulated_loss.item())\n",
    "\n",
    "        return np.mean(train_losses)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.EPOCHS):\n",
    "            train_loss = self.train_epoch()\n",
    "            self.writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{self.EPOCHS} Train Loss: {train_loss:.8f}\"\n",
    "            )\n",
    "\n",
    "        torch.save(self.model.state_dict(), \"unet_model.pth\")\n",
    "\n",
    "batch = next(iter(dataloader_train))\n",
    "input_vel_batch = batch[0][1]\n",
    "mask = torch.ones_like(input_vel_batch).to(device)\n",
    "mask[..., 0] = 0\n",
    "mask[..., -1] = 0\n",
    "model = ZetaUNet(order=ORDER, mask=mask).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = HybridLoss()\n",
    "trainer = PITrainerZeta(model, device, train_dataset, optimizer, criterion, writer, epochs=EPOCHS)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Physical Constraints Using the semi-implicit discretization\n",
    "\n",
    "The physical constraints can also be applied using the discretized form of the SWE. Network can output both velocity and elevations and the outputs can be forced to satisfy the following discretized equation\n",
    "\n",
    "$u^{n+1} = u^n - \\Delta t C_D\\frac{1}{h}u^n|u^n|- \\Delta t g (1-w_{\\textbf{imp}}) \\frac{\\partial \\zeta^{n}}{\\partial x}-\\Delta t g w_{\\textbf{imp}} \\frac{\\partial \\zeta^{n+1}}{\\partial x}$\n",
    "\n",
    "$\\zeta^{n+1} = \\zeta^n - \\Delta t (1-w_{\\textbf{imp}}) \\frac{\\partial h^n u^n}{\\partial x}-\\Delta t w_{\\textbf{imp}} \\frac{\\partial h^n u^{n+1}}{\\partial x}.\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Our tasks are as follows:\n",
    "\n",
    "1- Create the loss for updating the $\\zeta$ and $v$ values using the semi implicit discretization of SWE.\n",
    "\n",
    "2- Call the UNet model which outputs both field variables.\n",
    "\n",
    "3- Modify the training loop of our model.\n",
    "\n",
    "4- Run the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Physics informed loss function for Semi implicit discretized SWE\n",
    "class SemiImpLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SemiImpLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, zeta_dt, vel_dt, zeta_n, vel_n):\n",
    "        h_n = H0 + zeta_n\n",
    "        h_n_tilde = (h_n[..., :-1] + h_n[..., 1:]) / 2.0\n",
    "\n",
    "        zeta_nx = torch.diff(zeta_n, axis=-1) / DX\n",
    "        zeta_n1x = torch.diff(zeta_n + zeta_dt, axis=-1) / DX\n",
    "        zeta_nx = torch.nn.functional.pad(zeta_nx, (1, 1), \"constant\", 0)\n",
    "        zeta_n1x = torch.nn.functional.pad(zeta_n1x, (1, 1), \"constant\", 0)\n",
    "\n",
    "        h_n_tilde = torch.nn.functional.pad(h_n_tilde, (1, 1), \"constant\", 1)\n",
    "\n",
    "        U_nx = torch.diff(torch.mul(h_n_tilde, vel_n))\n",
    "        U_n1x = torch.diff(torch.mul(h_n_tilde, vel_n + vel_dt))\n",
    "\n",
    "        mass_loss = vel_dt + DT * (\n",
    "            CD * torch.div(torch.mul(vel_n, torch.abs(vel_n)), h_n_tilde)\n",
    "            + G * (1 - W_IMP) * zeta_nx\n",
    "            + G * W_IMP * zeta_n1x\n",
    "        )\n",
    "        mom_loss = zeta_dt + DT * (1 - W_IMP) * U_nx + DT * W_IMP * U_n1x\n",
    "\n",
    "        loss1 = torch.mean(mass_loss**2)\n",
    "        loss2 = torch.mean(mom_loss**2)\n",
    "\n",
    "        return loss1 + loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop for the hybrid model\n",
    "class PITrainerZetaV:\n",
    "    def __init__(self, model, device, train_dataset, optimizer, criterion, writer, epochs):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.train_dataset = train_dataset\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.writer = writer\n",
    "        self.EPOCHS = EPOCHS\n",
    "\n",
    "    def train_epoch(self):\n",
    "        train_losses = []\n",
    "        self.model.train()\n",
    "\n",
    "        init_zeta, init_vel = self.train_dataset.get_initial_conditions()\n",
    "        self.optimizer.zero_grad()\n",
    "        accumulated_loss = 0\n",
    "        input_zeta, input_vel = init_zeta.to(self.device), init_vel.to(self.device)\n",
    "\n",
    "        for _ in range(self.NUMTIME):\n",
    "            output_zeta_dt, output_vel_dt = self.model(input_zeta, input_vel, mask)\n",
    "            next_zeta = input_zeta + output_zeta_dt\n",
    "            next_vel = input_vel + output_vel_dt\n",
    "\n",
    "            unsupervised_loss = self.criterion(output_zeta_dt, output_vel_dt, input_zeta, input_vel)\n",
    "            accumulated_loss += unsupervised_loss\n",
    "\n",
    "            input_zeta, input_vel = next_zeta, next_vel\n",
    "\n",
    "        accumulated_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        train_losses.append(accumulated_loss.item())\n",
    "\n",
    "        return np.mean(train_losses)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.EPOCHS):\n",
    "            train_loss = self.train_epoch()\n",
    "            self.writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch+1}/{self.EPOCHS} Train Loss: {train_loss:.8f}\"\n",
    "            )\n",
    "\n",
    "        torch.save(self.model.state_dict(), \"unet_model.pth\")\n",
    "\n",
    "model = UnetMask(order=ORDER).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = SemiImpLoss()\n",
    "trainer = PITrainerZetaV(model, device, dataloader_train, optimizer, criterion, writer, epochs=EPOCHS)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Equivariant Convolutions?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
